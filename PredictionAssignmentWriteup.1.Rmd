Prediction Assignment Writeup
========================================================

# Project Description and Goal

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.  
These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.   
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  
In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.   
More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 
The training data for this project are available here:   
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
The test data are available here:   
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv    

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did.  
You will also use your prediction model to predict 20 different test cases. 

# Executive summary
The training data has over 19K rows and 160 columns:  
* identification fields for user / experiment
* data from accelerometers
* classe column containing the manner in which the exercise was performed; this output is used for training the supervised models used.
I performed the following operations for data cleansing:
* eliminate predictors with NAs values
* eliminate identification columns so only predictors + output remain
* eliminate correlated predictors; this operation proved to lower a little the accuracy of certain models used
* I also tried to eliminate skewness through Box-Cox pre-processing but finally I gave up because only 1 predictor seemed to be affected

I tried the following 3 models:
1. random forest model with cross validation with 10 folders and 51 as number of trees to grow; obtained accuracy - 99.2%
2. model based prediction with cross validation with 10 folders; obtained accuracy - 64%
3. boost with trees model with cross validation with 10 folders; obtained accuracy - 95%
So random forest model had the best results, predicting well all 20 test cases :) 
On second place came boost with trees model and in spite of 95% accuracy it failed to predict several of the 20 test cases :(
So I tried to "boost" model 2 and 3 by puting back the correlated predictors. The accuracy increased a little: 71% for model 2 and 96.8% for model 3, but inspite of this, boost with trees model predicted only 19 from the 20 test cases 
As a final remark, I'm sorry I passed over the 2000 words limit.

# Data 

As on windows there are R issues with downloading https://.... I downloaded in an working folder these 2 data files and loaded from there: 
```{r}
library(caret)
library(e1071)
library(corrplot)
#file_trainingAll <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv/pml-training.csv"
trainingAll <- read.csv(file="pml-training.csv", na.strings = c("NA", ""), stringsAsFactors=FALSE)
ncol(trainingAll)
nrow(trainingAll)

#file_testingAll <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv/pml-testing.csv"
testingAll <- read.csv(file="pml-testing.csv", na.strings = c("NA", ""), stringsAsFactors=FALSE)
ncol(testingAll)
nrow(testingAll)
```

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.; they have been very generous in allowing their data to be used for this kind of assignment.


# Exploratory Data Analysis
First, we need to get rid of columns with no data, NA-s or constant data with no variance.  
```{r}
testingAll <- testingAll[colSums(is.na(testingAll)) == 0]
trainingAll <- trainingAll[colSums(is.na(trainingAll)) == 0]
```

Then eliminate first 6 columns used for user/exercise identification:  
```{r}
testingAll <- testingAll[, -(1:7)]
trainingAll <- trainingAll[, -(1:7)]
```

Set output variable as factor:  
```{r}
trainingAll$classe <- as.factor(trainingAll$classe)
```
So, we have 52 numerical predictors and 1 categorical output variable to predict, resulting a classification problem :)

# Data preprocessing
Let's see how correlated are these numerical predictors
```{r}
correl_matrix <- cor(trainingAll[, -ncol(trainingAll)])

## Plot correlation matrix
corrplot(correl_matrix, order="hclust")
## findCorrelation from caret package is useful to delete variables with correlation larger than user-defined
## threshold
high_corr <- findCorrelation(correl_matrix,.8)
```
Let's delete highly correlated predictors:
```{r}
trainingAll_with_corr <- trainingAll
trainingAll <- trainingAll[,-high_corr]
ncol(trainingAll)
```
So, we remain with 40 predictors
Now let's see how many of these numerical predictors have nearly zero variance:
```{r}
zerovar <- nearZeroVar(trainingAll[, -ncol(trainingAll)])
zerovar
```
None of the remaining predictors has nearly zero variance.

Separate the predictors from the outcome variable:  
```{r}
predictors <- trainingAll[, -ncol(trainingAll)]
```

Use caret's preProcess function to transform for skewness:  
```{r}
predictors_processed <- preProcess(predictors, method = "BoxCox")
predictors_boxCox <- predict(predictors_processed, predictors)
 
## We can check which variable has been transformed using boxcox transformation
predictors_skewness <- sapply(predictors,skewness)
predictors_skewness_2 <- sapply(predictors_boxCox,skewness)
predictors_skewness - predictors_skewness_2
```
As only  magnet_belt_y predictor has been transformed by the Box-Cox pre-process we can forget about Box-Cox processing :)

# Split available trainingAll dataframe in 2 sets: for training and testing models
```{r}
set.seed(1234)
inTrain <- createDataPartition(y = trainingAll$classe, p=0.7, list=FALSE)
training <- trainingAll[inTrain, ]
testing <- trainingAll[-inTrain, ]
```

# Model fitting

## 1. random forest model
Try a random forest model with cross validation with 10 folders and 51 as number of trees to grow where 51 is a number close to the number of predictors :  
```{r message=FALSE}
# train random forest predictive model with cross validation on 10 folds
# and Number of trees to grow set to predictors number
model_rf <- train(classe ~., data = training, method = "rf", 
                 trControl = trainControl(method = "cv", number = 10), 
                 ntree = 51, importance = TRUE)

# apply Random Forest model to testing set
model_rf_prediction <- predict(model_rf, testing)

# compute confusion matrix for rf model for accuracy and out of sample error
confusion_matrix <- confusionMatrix( model_rf_prediction, testing$classe)
confusion_matrix$table
model_rf_accuracy <- confusion_matrix$overall["Accuracy"][[1]]
model_rf_accuracy

# estimated out-of-sample error 
out_of_sample_error_rf <- 1 - model_rf_accuracy
out_of_sample_error_rf
```

## 2. model based prediction
Try a model based prediction with cross validation with 10 folders :  
```{r message=FALSE}
model_lda <- train(classe ~., data = training, method = "lda", 
                 trControl = trainControl(method = "cv", number = 10))

# apply lda model to testing set
model_lda_prediction <- predict(model_lda, testing)

# compute confusion matrix for lda model
confusion_matrix <- confusionMatrix( model_lda_prediction, testing$classe)
confusion_matrix$table
model_lda_accuracy <- confusion_matrix$overall["Accuracy"][[1]]
model_lda_accuracy

# estimated out-of-sample error 
out_of_sample_error <- 1 - model_lda_accuracy
out_of_sample_error

```

## 3. boost with trees model
Try a boost with trees model  with cross validation with 10 folders :  
```{r message=FALSE}
model_gbm <- train(classe ~., data = training, method = "gbm", verbose=FALSE,
                 trControl = trainControl(method = "cv", number = 10))

# apply gbm model to testing set
model_gbm_prediction <- predict(model_gbm, testing)

# compute confusion matrix for gbm model
confusion_matrix <- confusionMatrix( model_gbm_prediction, testing$classe)
confusion_matrix$table
model_gbm_accuracy <- confusion_matrix$overall["Accuracy"][[1]]
model_gbm_accuracy

# estimated out-of-sample error 
out_of_sample_error <- 1 - model_gbm_accuracy
out_of_sample_error

```


# Predict the 20 requested tests with the best of the tried models
```{r}
final_predictions_based_on_rf <- predict(model_rf, testingAll)
final_predictions_based_on_lda <- predict(model_lda, testingAll)
final_predictions_based_on_gbm <- predict(model_gbm, testingAll)
```
Prediction done with random forest model:  
```{r}
final_predictions_based_on_rf
```

Prediction done with lda model:  
```{r}
final_predictions_based_on_lda
```

Prediction done with boost with trees model:  
```{r}
final_predictions_based_on_gbm
```
As you can see, all 3 predictions are not identical and I used the prediction generated by random forest model with cross validation with 10 folders and 51 as number of trees because it has the best accuracy:
```{r}
round(model_rf_accuracy, 3)
```

# Prediction Assignment Submission

```{r}
pml_write_files = function(x, folder){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./",folder,"/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(final_predictions_based_on_rf, "rf")
```

# Another try with lda and gbm models but without eliminating correlated predictors
As the 3 models have different predictions I try again lda and gbm models but without eliminating correlated predictors:  
```{r}
inTrain <- createDataPartition(y = trainingAll_with_corr$classe, p=0.7, list=FALSE)
training <- trainingAll_with_corr[inTrain, ]
testing <- trainingAll_with_corr[-inTrain, ]
```
## 2.1 model based prediction without eliminating correlated predictors
Try a model based prediction with cross validation with 10 folders :  
```{r message=FALSE}
model_lda <- train(classe ~., data = training, method = "lda", 
                 trControl = trainControl(method = "cv", number = 10))

# apply lda model to testing set
model_lda_prediction <- predict(model_lda, testing)

# compute confusion matrix for lda model
confusion_matrix <- confusionMatrix( model_lda_prediction, testing$classe)
confusion_matrix$table
model_lda_accuracy <- confusion_matrix$overall["Accuracy"][[1]]
model_lda_accuracy

# estimated out-of-sample error 
out_of_sample_error <- 1 - model_lda_accuracy
out_of_sample_error

```

## 3.1 boost with trees model without eliminating correlated predictors
Try a boost with trees model  with cross validation with 10 folders :  
```{r message=FALSE}
model_gbm <- train(classe ~., data = training, method = "gbm", verbose=FALSE,
                 trControl = trainControl(method = "cv", number = 10))

# apply gbm model to testing set
model_gbm_prediction <- predict(model_gbm, testing)

# compute confusion matrix for gbm model
confusion_matrix <- confusionMatrix( model_gbm_prediction, testing$classe)
confusion_matrix$table
model_gbm_accuracy <- confusion_matrix$overall["Accuracy"][[1]]
model_gbm_accuracy

# estimated out-of-sample error 
out_of_sample_error <- 1 - model_gbm_accuracy
out_of_sample_error

final_predictions_based_on_lda <- predict(model_lda, testingAll)
final_predictions_based_on_gbm <- predict(model_gbm, testingAll)
```

Prediction done with lda model:  
```{r}
final_predictions_based_on_lda
```

Prediction done with boost with trees model:  
```{r}
final_predictions_based_on_gbm
```




